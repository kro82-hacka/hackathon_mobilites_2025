{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Hackathon mobilités_logo simple.jpg\" alt=\"Logo Hackathon 2025\" width=\"400\"/>\n",
    "\n",
    "## Snippets de code \n",
    "\n",
    "Ces snippets de code vont vous permettre de gagner du temps dans la prise en main des ressources à disposition et de l'écosystème data d'île-de-France Mobilités. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sommaire\n",
    "\n",
    "- [Appel d'API PRIM — Instanciation API](#appel-dapi-prim)\n",
    "  - [Nomenclature pour les appels API](#nomenclature-pour-les-appels-api)\n",
    "    - [Code ligne vers identifiant SIRI](#code-ligne-vers-identifiant-de-ligne-siri)\n",
    "    - [Code arrêt vers identifiant d'arrêt SIRI](#code-arrêt-vers-identifiant--darrêt-siri)\n",
    "  - [Appel d'API PRIM — Exemple de calcul d'itinéraire](#exemple-de-demande-de-calcul-ditinéraire)\n",
    "  - [Appel d'API PRIM — Autres exemples d'appels vers des endpoints](#autres-exemples-dappels-vers-des-endpoints)\n",
    "- [Onyxia — Connexion au stockage](#connexion-au-stockage)\n",
    "  - [Lecture S3](#lecture-dun-fichier-depuis-lespace-de-stockage-partagé-vers-une-dataframe-pandas)\n",
    "  - [Ecriture dataframe vers S3](#ecriture-dun-fichier-depuis-une-dataframe-pandas-vers-lespace-de-stockage-partagé)\n",
    "  - [Ecriture fichiers vers S3](#ecriture-de-plusieurs-fichiers-au-sein-dun-dossier-vers-lespace-de-stockage-partagé)\n",
    "- [Utilisation de modèles ouverts sur Hugging Face en local](#utilisation-de-modèles-ouverts-sur-hugging-face-en-local)\n",
    "- [Utilisation de modèles OpenAI déployés par IDFM sur Azure](#utilisation-de-modèles-deployés-par-idfm-sur-azure)\n",
    "  - [Appel d'un LLM](#appel-dun-llm)\n",
    "  - [Appel d'un modèle d'embedding](#appel-dun-modèle-dembedding)\n",
    "\n",
    "*Cliquez sur un titre (si votre visualiseur de notebook supporte les ancres) pour accéder à la section.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appel d'API PRIM\n",
    "\n",
    "Île de France Mobilités met à disposition une plateforme pour interroger ses API d'information voyageurs. Cette plateforme nécessite la création d'un token et comporte quelques subtilités dans les codifications des lignes. Des éléments sont fournis ci-dessous pour vous en faciliter l'utilisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/api_prim.png\" alt=\"Token authentification PRIM\" width=\"900\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Informations API\n",
    "API_BASE = \"https://prim.iledefrance-mobilites.fr/marketplace/\"\n",
    "API_KEY = \"your-api-key\"  # remplacer par votre clé\n",
    "HEADERS = {\"Accept\": \"application/json\", \"apikey\": API_KEY}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nomenclature pour les appels API\n",
    "##### Code ligne vers identifiant de ligne SIRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de l'id_ligne du référentiel des lignes en id compatible avec les API\n",
    "# /!\\ les \"deux points\" finaux sont indispensables\n",
    "\n",
    "code_ligne_idfm = \"C01742\"  # RER A\n",
    "id_ligne_idfm = \"STIF:Line::{id_ligne_idfm}:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code arrêt vers identifiant  d'arrêt SIRI\n",
    "_Note : les identifiant d'arrêts à prendre en compte sont en général les id de zones d'arrêt._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de ZdAId du référentiel des arrêts en id compatible avec les API\n",
    "# /!\\ les \"deux points\" finaux sont indispensables\n",
    "\n",
    "code_zone_arret_idfm = \"42135\"  # Les Dix Arpents\n",
    "id_arret_idfm = \"STIF:monomodalStopPlace::{code_zone_arret_idfm}:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple de demande de calcul d'itinéraire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = API_BASE + \"v2/navitia/journeys\"\n",
    "\n",
    "# Coordonnées (lon, lat) — des chaînes conviennent pour le tutoriel\n",
    "ORIGIN_LON = \"2.33792\"\n",
    "ORIGIN_LAT = \"48.85827\"\n",
    "DEST_LON = \"2.3588523\"\n",
    "DEST_LAT = \"48.9271087\"\n",
    "\n",
    "# Date/heure pour le trajet (format : YYYYMMDDTHHMMSS)\n",
    "TRIP_DATETIME = \"20241121T073000\"\n",
    "\n",
    "# Construire l'URL (Navitia attend lon;lat encodé en lon%3B%20lat)\n",
    "FROM_PARAM = f\"{ORIGIN_LON}%3B%20{ORIGIN_LAT}\"\n",
    "TO_PARAM = f\"{DEST_LON}%3B%20{DEST_LAT}\"\n",
    "URL = f\"{API_URL}?from={FROM_PARAM}&to={TO_PARAM}&datetime={TRIP_DATETIME}\"\n",
    "\n",
    "# Afficher l'URL pour que le lecteur voie comment elle est construite\n",
    "print(\"Aperçu de l'URL Navitia :\")\n",
    "print(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exécuter la requête inline (style tutoriel simple)\n",
    "try:\n",
    "    resp = requests.get(URL, headers=HEADERS)\n",
    "    print(\"Statut HTTP :\", resp.status_code)\n",
    "\n",
    "    if resp.status_code == 200:\n",
    "        data = resp.json()\n",
    "        # Aplatir le JSON de premier niveau pour inspection\n",
    "        df = pd.json_normalize(data)\n",
    "        print(\"Clés de premier niveau :\", list(data.keys()))\n",
    "\n",
    "        # Si des 'journeys' sont présents, inspecter le premier\n",
    "        if isinstance(data, dict) and data.get(\"journeys\"):\n",
    "            print(\"Premier trajet (aplatit) :\")\n",
    "            display(pd.json_normalize(data[\"journeys\"][0]))\n",
    "        else:\n",
    "            print(\"Aucun trajet ('journeys') retourné dans la réponse.\")\n",
    "    else:\n",
    "        print(\"Réponse non-200, corps (400 premiers caractères) :\")\n",
    "        print(resp.text[:400])\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"La requête a échoué :\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autres exemples d'appels vers des endpoints\n",
    "\n",
    "Ci-dessous un exemple avec l'endpoint general-message qui renvoie des messages info trafic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_LIGNE = \"C01742\"  # Exemple : RER A\n",
    "API_URL = f\"{API_BASE}general-message?LineRef=STIF%3ALine%3A%3A{ID_LIGNE}%3A\"\n",
    "\n",
    "# Affichage rapide pour vérification (utile en mode tutoriel)\n",
    "print(\"Requête vers :\", API_URL)\n",
    "\n",
    "# --- Requête inline et traitement minimal ---\n",
    "try:\n",
    "    response = requests.get(API_URL, headers=HEADERS)\n",
    "    print(\"Statut HTTP :\", response.status_code)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Parser le JSON et l'afficher (ou l'extraire selon le besoin)\n",
    "        json_data = response.json()\n",
    "        print(\"Réponse JSON (extrait) :\")\n",
    "        # Affiche la structure JSON (ou un extrait) pour inspection\n",
    "        print(json.dumps(json_data, indent=2, ensure_ascii=False)[:1000])\n",
    "    else:\n",
    "        print(\"Échec de la requête HTTP. Statut :\", response.status_code)\n",
    "        print(\"Réponse (début) :\", response.text[:400])\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Une erreur s'est produite lors de l'appel HTTP :\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onyxia\n",
    "#### Connexion au stockage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/acces_s3_onyxia.png\" alt=\"Accès S3 Onyxia\" width=\"900\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "# Remplacer par vos propres informations d'identification et paramètres\n",
    "ACCESS_KEY = \"your-access-key\"\n",
    "SECRET_KEY = \"your-secret-key\"\n",
    "SESSION_TOKEN = \"your-session-token\"\n",
    "REGION = \"fr-central\"\n",
    "ENDPOINT_URL = \"minio.data-platform-self-service.net\"\n",
    "\n",
    "BUCKET = \"dlb-hackathon\"\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"https://\" + ENDPOINT_URL,\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY,\n",
    "    aws_session_token=SESSION_TOKEN,\n",
    "    region_name=REGION,\n",
    ")\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=\"dlb-hackathon\")\n",
    "if \"Contents\" in response:\n",
    "    for obj in response[\"Contents\"]:\n",
    "        if obj[\"Key\"].startswith(\"datasets-diffusion/2025\"):\n",
    "            print(obj[\"Key\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lecture d'un fichier depuis l'espace de stockage partagé vers une dataframe pandas\n",
    "\n",
    "_RAPPEL : pour plus d'informations sur un jeu de données spécifique (structure, source, exemple de cas d'usage en accord avec les défis, etc.), prenez le temps de lire le README.md présent dans le dossier associé._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_KEY_S3 = \"/datasets-diffusion/2024/ascenseurs_historique_etat/RELEVES_ETATS_ASCENSEURS_SNCF_RATP_2021-2024.csv\"\n",
    "\n",
    "response = s3.get_object(Bucket=BUCKET, Key=FILE_KEY_S3)\n",
    "status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "if status == 200:\n",
    "    df = pd.read_csv(response.get(\"Body\"))\n",
    "    print(\"Aperçu des données importées :\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(f\"Erreur lors de l'importation des données. Statut HTTP: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ecriture d'un fichier depuis une dataframe pandas vers l'espace de stockage partagé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_KEY_S3 = \"tests_io_onyxia/books.csv\"\n",
    "books_df = pd.DataFrame(\n",
    "    data={\"Title\": [\"Book I\", \"Book II\", \"Book III\"], \"Price\": [56.6, 59.87, 74.54]},\n",
    "    columns=[\"Title\", \"Price\"],\n",
    ")\n",
    "\n",
    "with io.StringIO() as csv_buffer:\n",
    "    books_df.to_csv(csv_buffer, index=False)\n",
    "    response = s3.put_object(Bucket=BUCKET, Key=FILE_KEY_S3, Body=csv_buffer.getvalue())\n",
    "\n",
    "    status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "    if status == 200:\n",
    "        print(\"Fichier écrit avec succès dans l'espace de stockage partagé.\")\n",
    "    else:\n",
    "        print(f\"Erreur lors de l'écriture du fichier. Statut HTTP: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ecriture de plusieurs fichiers au sein d'un dossier vers l'espace de stockage partagé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "LOCAL_FOLDER = \"C:/Users/.../exploration_dept_data/0_donnees_2025\"\n",
    "PREFIX = \"datasets-diffusion/2025\"\n",
    "\n",
    "paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "for root, dirs, files in os.walk(LOCAL_FOLDER):\n",
    "    for filename in files:\n",
    "        local_path = os.path.join(root, filename)\n",
    "        relative_path = os.path.relpath(local_path, LOCAL_FOLDER)\n",
    "        s3_key = f\"{PREFIX}/{relative_path.replace(os.sep, '/')}\"\n",
    "        \n",
    "        print(f\"Uploading {local_path} → s3://{BUCKET}/{s3_key}\")\n",
    "        s3.upload_file(local_path, BUCKET, s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilisation de modèles ouverts sur Hugging Face en local\n",
    "\n",
    "La plateforme permet d'explorer l'utilisation de petits modèles. Ces modèles ne sont exécutés qu'avec du CPU car la plateforme ne dispose pas de GPU. Il est recommandé d'explorer des modèles de petite taille, avec moins de 500 millions ou un milliard de paramètres, car au-delà, le temps d'exécution serait trop long et cela perturberait le fonctionnement général de la plateforme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Cette instruction minimale permet de sélectionner le modèle et son niveau de compression, le téléchargement le paramétrage et la mise en mémoire (CPU), sont automatiques :\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"bartowski/gemma-2-2b-it-GGUF\",  # Plus gros dépôt de LLM libres de Hugging face : https://huggingface.co/bartowski\n",
    "    filename=\"gemma-2-2b-it-Q6_K_L.gguf\",  # Attention a prendre des modèles de taille <= 6Go pour des performances raisonnables, pas de garantie de fonctionnement au delà\n",
    "    n_ctx=4096,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Une fois le LLM chargé (environ une minute par Go), il peut être appelé, via une fonction *completion, ici pour un chatbot :\n",
    "llm_response = llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"###SYSTEM : You are the Capital assistant. I give you a country, you give me the capital and try to retrieve the country iso code. ### FORMAT : Capital=<name of the Capital> CODE=<ISO CODE 2 CHARS>\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"OK, I understand. If you say France, i say Capital=Paris CODE=FR\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Germany\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# La réponse du LLM est pleine de métadonnées, mais le message texte peut être extrait de cette manière simple :\n",
    "print(llm_response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilisation de modèles deployés par IDFM sur Azure\n",
    " \n",
    "Documentation utile :\n",
    "- https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/responses?tabs=python-key\n",
    "- https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/chatgpt\n",
    " \n",
    "Les urls des endpoints de vos équipes respectives ainsi que les tokens d'authentification associés vous seront communiqués via Slack.\n",
    " \n",
    "Ci-après la liste des modèles et versions disponibles :\n",
    "* o4-mini / 2025-04-16\n",
    "* gpt-4o-mini / 2024-07-18\n",
    "* Llama-4-Maverick-17B-128E-Instruct-FP8 / 1\n",
    "* text-embedding-3-large / 1\n",
    " \n",
    "Les **quotas de requêtes** sont les suivants :\n",
    "- LLM : 50000 tokens/minute\n",
    "- Embeddings : 100000 tokens/minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appel d'un LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "endpoint = \"votre_endpoint_d_equipe\"+\"openai/v1/\"\n",
    "api_key = \"votre_cle_d_equipe\"\n",
    "\n",
    "model_name = \"gpt-4o-mini\" #\"Llama-4-Maverick-17B-128E-Instruct-FP8\" #\"gpt-4o-mini\" #\"o4-mini\" #\"Llama-4-Maverick-17B-128E-Instruct-FP8\"\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=f\"{endpoint}\",\n",
    "    api_key=api_key\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What were the capitals of France?\",\n",
    "        }\n",
    "    ],\n",
    "    max_completion_tokens=13107,\n",
    "    temperature=1,\n",
    "    top_p=1.0,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0,\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appel d'un modèle d'embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "endpoint = \"votre_endpoint_d_equipe\"+\"openai/v1/\"\n",
    "api_key = \"votre_cle_d_equipe\"\n",
    "\n",
    "model_name =model_name = \"text-embedding-3-large\"\n",
    " \n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=api_key\n",
    ")\n",
    "response = client.embeddings.create(\n",
    "    input=[\"first phrase\",\"second phrase\",\"third phrase\"],\n",
    "    model=model_name\n",
    ")\n",
    "for item in response.data:\n",
    "    length = len(item.embedding)\n",
    "    print(\n",
    "        f\"data[{item.index}]: length={length}, \"\n",
    "        f\"[{item.embedding[0]}, {item.embedding[1]}, \"\n",
    "        f\"..., {item.embedding[length-2]}, {item.embedding[length-1]}]\"\n",
    "    )\n",
    "print(response.usage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
